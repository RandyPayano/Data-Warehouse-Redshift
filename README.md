# Data Engineering NanoDegree

## Author
Randy Payano [Linkedin](https://www.linkedin.com/in/randy-payano/)

### Project 3: Data Warehouse with Amazon Redshift

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud.
Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, I will build an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to continue finding insights in what songs their users are listening to.

Finally I'll test the database and ETL pipeline by running queries given by the analytics team from Sparkify and compare the results with their expected results.

### Project Structure
```
Data-Warehouse--Redshift
│   README.md              # project description
|   requirements.txt       # python dependencies
|   func.cfg               # configuration file
|   etl.py                 # script to run entire ETL process
|
└───lib                         # functions library 
|   |
|   |  config_update.py         # updates configuration file
│   │  create_aws_resources.py  # creates aws resources
|   |  redshift.py              # functions to create and delete redshift cluster
|   |  iam_role.py              # functions to create and delete iam role
|   |  vpc_security_group.py    # functions to create and delete security groups
|   |  sql_queries.py           # function to define all sql objects
|   |  create_table.py          # creates table schema using sql_queries defined objects
|   |  validation.py            # Validates data load
|   |  delete_aws_resources.py  # deletes aws resources
```

### Requirements for running locally
- Python3
- AWS Account (Key and Secret and input them in func.cfg in the [AWS] section)

### Datasets

I will be working with two datasets that reside in AWS S3. These links are preconfigured in the configuration file:

- Song data: `s3://udacity-dend/song_data`
- Log data: `s3://udacity-dend/log_data`

Log data json path: `s3://udacity-dend/log_json_path.json`

<br />

**Song Dataset:**

It's a subset of real data from the [Million Song Dataset].
Each file is in JSON format and contains metadata about a song and the artist of that song
The first dataset is a subset of real data from the [Million Song](https://labrosa.ee.columbia.edu/millionsong/) Dataset . 
Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three 
letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
Below is an example of how these files look like:

```
{
    "num_songs":1,
    "artist_id":"ARD7TVE1187B99BFB1",
    "artist_latitude":null,
    "artist_longitude":null,
    "artist_location":"California - LA",
    "artist_name":"Casual",
    "song_id":"SOMZWCG12A8C13C480",
    "title":"I Didn't Mean To",
    "duration":218.93179,
    "year":0
 }
```

**Log dataset:**

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim)
based on the songs in the dataset above. These simulate app activity logs from an imaginary
music streaming app based on configuration settings. Here are the file paths to these files:

```log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

Below is an example of how these files look like:

```
{
   "artist":null,
   "auth":"Logged In",
   "firstName":"Walter",
   "gender":"M",
   "itemInSession":0,
   "lastName":"Frye",
   "length":null,
   "level":"free",
   "location":"San Francisco-Oakland-Hayward, CA",
   "method":"GET",
   "page":"Home",
   "registration":1540919166796.0,
   "sessionId":38,
   "song":null,
   "status":200,
   "ts":1541105830796,
   "userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"",
   "userId":"39"
}
```
### Database Schema

Using the song and log datasets, you'll need to create a star schema optimized for queries on song play analysis.

This includes the following tables.

### Fact Table
```
• songplays - records in log data associated with song plays i.e. records with page NextSong
  table schema: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
```
### Dimension Tables
```
• users - users in the app
  table schema: user_id, first_name, last_name, gender, level

• songs - songs in music database
  table schema: song_id, title, artist_id, year, duration

• artists - artists in music database
  table schema: artist_id, name, location, latitude, longitude

• time - timestamps of records in songplays broken down into specific units
  table schema: start_time, hour, day, week, month, year, weekday
```
### Instructions for running locally
1) Clone this repository 
```
git clone https://github.com/drobim-data-engineering/Data-Warehouse-with-Redshift.git
```
#### Create Python Virtual Environment
```
python3 -m venv venv             # create virtualenv
source venv/bin/activate         # activate virtualenv
pip install -r requirements.txt  # install requirements
```

#### Configure the (AWS) section in func.cfg
```
KEY = User Access Key
SECRET = User Secret Key
***Remember to not share these details with others***
```

#### Run ETL (etl.py)
```

This file will create all AWS resources, connect to our database, create the necessary tables and load them appropriately.
After the ETL process is complete, you will be asked to validate the server by running queries against the database. You can choose 
from 3 questions from numbers 1-3. After validation is complete the ETL process will delete all resources as this is just an exercise 
that could cost you real money! 
```
